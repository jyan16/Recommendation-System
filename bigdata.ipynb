{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recommendation System with Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOAL\n",
    "1. Setup Google cloud environment and Cloud MongoDB\n",
    "2. Interactive will HDFS using shell command\n",
    "3. Process batch data with Spark\n",
    "4. Use NoSQL to interact with MongoDB\n",
    "5. Manipulate stream data with Spark Streaming\n",
    "6. Generate message queue via Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Philosophy of this system\n",
    "This movie recommendation system based on [Amazon.com recommend system](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf). This is a good way to recommend items on the top of massive data, cause you can easily separate offline training and online processing. the following figure shows the main architecture:\n",
    " <img src=\"document/bigdata.jpg\" width = \"400\" height = \"600\" alt=\"what\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data will be stored in HDFS, Hadoop Distributed File System. From the batch training part, Spark loads raw data from HDFS, and calculates movies' similarity to each other. All the neccessary information will be stored in MongoDB, including movie id-title pair and movies similarity. As for the streaming part, it uses Apache Kafka to manage message queue, which contains user's real-time ratings. Spark streaming component will process these rating stream, generate immediate recommendation, and update user watching list. As you can see, the offline training and streaming processing are independently maintained, in code/batch_pipeline.py and code/streaming.py separately.\n",
    "\n",
    "You can find a more detailed guide in Amazon.com [US patent paper](https://www.google.com/patents/US6266649). I roughly follow their figure 3 for batch pipeline, and figure 2 for streaming pipeline.\n",
    "\n",
    "The whole system is built on Google Cloud cluster. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <img src=\"document/workflow.jpeg\" width = \"1000\" height = \"800\" alt=\"what\" align=center />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
