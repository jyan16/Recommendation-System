{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 5: Recommendation System with Big Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview\n",
    "I LOVE MOVIE corp has been more and more popular since their last collaboration with you. To make more money, just like all the other companies, they plan to build a movie recommendation system for their customers. However, because they are too popular, well, and have plenty of money, they decide to use Big Data engines and build their system on Google Cloud. As their technology counselor, you will fulfill this requirement in this assginment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOAL\n",
    "1. Setup Google cloud environment and Cloud MongoDB\n",
    "2. Interactive will HDFS using shell command\n",
    "3. Process batch data with Spark\n",
    "4. Use NoSQL to interact with MongoDB\n",
    "5. Manipulate stream data with Spark Streaming\n",
    "6. Generate message queue via Kafka"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Philosophy of this system\n",
    "We will build our movie recommendation system based on [Amazon.com recommend system](https://www.cs.umd.edu/~samir/498/Amazon-Recommendations.pdf). This is a good way to recommend items on the top of massive data, cause you can easily separate offline training and online processing. the following figure shows the main architecture for your product:\n",
    " <img src=\"document/bigdata.jpg\" width = \"400\" height = \"600\" alt=\"what\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw data will be stored in HDFS, Hadoop Distributed File System. From the batch training part, Spark loads raw data from HDFS, and calculates movies' similarity to each other. All the neccessary information will be stored in MongoDB, including movie id-title pair and movies similarity. As for the streaming part, we use Apache Kafka to manage message queue, which contains user's real-time ratings. Spark streaming component will process these rating stream, generate immediate recommendation, and update user watching list. As you can see, the offline training and streaming processing are independently maintained, in code/batch_pipeline.py and code/streaming.py separately.\n",
    "\n",
    "You can find a more detailed guide in Amazon.com [US patent paper](https://www.google.com/patents/US6266649). We will roughly follow their figure 3 for batch pipeline, and figure 2 for streaming pipeline.\n",
    "\n",
    "We will build the whole system on Google Cloud cluster. This is because cloud offers a way to build system with high scalability. Imaging I LOVE MOVIE company has more and more customers, we can simply add more nodes by a fews clickes on Google Cloud dashboard and things will be done. Also, if the company loses a lot of users, we can decrease node number on cluster too. In this assignment, however, we will use this powerful cloud cluster to process data of several Megabytes, as you know, the only reason for us to do so is because Google will charge too much for heavy usage of computation resources.\n",
    "\n",
    "** Development Cycle ** It is inconvenient to develop code that runs on cloud, especially when we don't setup local environment and use pyspark, which is not a normal Python program. To simplify your life, do the following steps repeatedly when developing your code:\n",
    "\n",
    "1. use mapreduce class we introduced in lab2 to tune your mapper and reducer locally, make sure each function performs correctly.\n",
    "2. follow steps in lab1 to submit your code to cloud, remember to use '> output.txt' command that can help you separate log info and program output.\n",
    "3. if any bugs happen, analyze error info at output.txt, and update your local code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab1: Setup Cloud Environment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab, we will follow the script to setup cluster. you will get to know how each shell command work. Finally, we will execute testcase1.py in your cluster, which contains all tools you need in batch data pipeline.\n",
    "\n",
    "#### Create cluster \n",
    "\n",
    "We will use Google Dataproc to establish your cluster. Since you already setup the [gcloud SDK](https://cloud.google.com/sdk/gcloud/) in assignment 3, let's start by running shell command shell/init_cluster.sh:\n",
    "~~~~\n",
    "gcloud dataproc clusters create recommend --initialization-actions \\\n",
    "\"gs://dataproc-initialization-actions/jupyter/jupyter.sh,gs://dataproc-initialization-actions/kafka/kafka.sh\" \\\n",
    "--master-machine-type n1-standard-1 --num-masters 3 --master-boot-disk-size 50GB \\\n",
    "--worker-machine-type n1-standard-2 --num-workers 2 --worker-boot-disk-size 50GB\n",
    "~~~~\n",
    "In this command, we create a cluster named ** recommend ** in Dataproc. --initialization-actions specify how we wish to initialize our cluster, in another word, install and run what software in this cluster. Here we choose to init it with two actions, jupyter.sh and kafka.sh. [jupyter.sh](https://github.com/GoogleCloudPlatform/dataproc-initialization-actions/tree/master/jupyter) install mini conda, a simplified version of anaconda, and lunch jupyter notebook with pyspark kernel support for you in port 8123 (however, we won't use jupyter notebook to code spark, it's really inconvenient). [kafka.sh](https://github.com/GoogleCloudPlatform/dataproc-initialization-actions/tree/master/kafka) install kafka in our cluster. \n",
    "\n",
    "The following two lines specify master and worker configuration. Here, our cluster has three masters, with n1-standard-1 type of CPU and 50GB boot disk size, and 2 workers, with n1-standard-2 type of CPU and 50GB boot disk size. n1-standard-1 CPU has one core, while n1-standard-2 CPU has two cores. Therefore, there are totally 7 cores in your cluster, which just fit into Google Cloud limit of 8 cores. You must be really confused why we have three masters. This is because kafka.sh need you to install zookeeper in advance, or have a high availability cluster (three master nodes would safisfy this requirement), either way would be fine.\n",
    "\n",
    "#### Setup proxy access \n",
    "\n",
    "Next, we will use shell/connect.sh to setup proxy which allows you to connect to cluster via browser. You need to install Chrome in advance.\n",
    "~~~~\n",
    "gcloud compute ssh --zone=us-central1-c --ssh-flag=\"-D 1080\" --ssh-flag=\"-N\" --ssh-flag=\"-n\" recommend-m-0 &\n",
    "\n",
    "/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --proxy-server=\"socks5://localhost:1080\" \\\n",
    "--host-resolver-rules=\"MAP * 0.0.0.0 , EXCLUDE localhost\" --user-data-dir=/tmp/\n",
    "~~~~\n",
    "\n",
    "There are two commands in this .sh file. The first one create proxy to master-0 node of your recommend cluster, which specify region with --zone and port with --ssh-flag. The symbol & in the end means run this command backend. The second command launches your Chrome browser and send all it's request via proxy port. You can find detailed explanation from [here](https://cloud.google.com/dataproc/docs/concepts/cluster-web-interfaces).\n",
    "\n",
    "#### Setup MongoDB Cloud \n",
    "\n",
    "This step would be relatively easy, visit [mlab website](https://mlab.com/welcome/?gclid=EAIaIQobChMI4ezHnojm1QIVExuBCh2v6QbhEAAYASAAEgKbqPD_BwE) to sign up your account. 500MB free plan would be sufficient for you. Create a database named ** netflix **, all of our future work will be done in it. You will see your MongoDB URI at the upper left of mlab website, which will be used later for connecting to MongoDB in Python.\n",
    "\n",
    "#### Run your program on Cloud\n",
    "Now that all the tools we need has been setup. Let's now run our first task on the cloud! This program will create collection ** rating ** in your netflix database, which contains the user list that have rated each movie. In order to at least check grammar of our code locally, let's install the following two library in your computer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymongo in /Users/jinyan/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: pyspark in /Users/jinyan/anaconda/lib/python3.6/site-packages\n",
      "Requirement already satisfied: py4j==0.10.4 in /Users/jinyan/anaconda/lib/python3.6/site-packages (from pyspark)\n"
     ]
    }
   ],
   "source": [
    "!pip install pymongo\n",
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains code in code/batch_test.py file. It uses Spark and SparkSQL together. You may refer to this code when you create your own recommendation system later.\n",
    "\n",
    "** note ** this code cannot run locally, it sits in this Jupyter Notebook just for introduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import argparse\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='test cloud setup')\n",
    "    parser.add_argument('-mongo', help='MongoDB database URI')\n",
    "    parser.add_argument('-r', help='path to input ratings.csv', default='../data/ratings.csv')\n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "def mapper1(record):\n",
    "    \"\"\"\n",
    "    :param record: (user_id, movie_id)\n",
    "    :return:       (movie_id, user_id)\n",
    "    \"\"\"\n",
    "    return (record[1], [record[0]])\n",
    "\n",
    "def reducer(a, b):\n",
    "    return a + b\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    # parse the arguments\n",
    "    args = parse_args()\n",
    "    \n",
    "    # Spark initialization\n",
    "    # sc is Spark context, while ssc is SparkSQL context. You may simply regard them as connector to spark.\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"data1030\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", args.mongo) \\\n",
    "        .config(\"spark.mongodb.output.uri\", args.mongo) \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    ssc = SQLContext(sc)\n",
    "    \n",
    "    # load ratings.csv into dataframe rating_df\n",
    "    rating_df = ssc.read.format('csv') \\\n",
    "        .option('header', 'true') \\\n",
    "        .option('inferschema', 'true').option('mode', 'DROPMALFORMED').load(args.r)\n",
    "    # register rating_df as temporary table, only then can you use SparkSQL to query it\n",
    "    ssc.registerDataFrameAsTable(rating_df, 'watch')\n",
    "    \n",
    "    # use SparkSQL to get user_id and movie_id, this function returns a dataframe\n",
    "    tmp_df = ssc.sql('''SELECT userId AS user_id, movieId AS movie_id\n",
    "                        FROM watch''')\n",
    "    \n",
    "    # use mapreduce to generate (movie_id, user_list) pair\n",
    "    tmp_rdd = tmp_df.rdd.map(tuple) \\\n",
    "        .map(mapper1) \\\n",
    "        .reduceByKey(reducer)\n",
    "        \n",
    "    table_df = spark.createDataFrame(tmp_rdd, ['movie_id', 'user_list'])\n",
    "    \n",
    "    # print out schema and 5 rows\n",
    "    table_df.printSchema()\n",
    "    pprint(table_df.take(5))\n",
    "    \n",
    "    # load data into mongodb database\n",
    "    table_df.write.format(\"com.mongodb.spark.sql.DefaultSource\") \\\n",
    "        .option(\"collection\", \"watch\") \\\n",
    "        .mode(\"overwrite\").save()\n",
    "    \n",
    "    # drop temp table \n",
    "    ssc.dropTempTable('watch')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we mixed use normal [Spark](https://spark.apache.org/docs/0.9.0/python-programming-guide.html) and [SparkSQL](https://spark.apache.org/docs/latest/sql-programming-guide.html), which can make our coding more easily. The dataframe in Spark is really similar to dataframe in pandas, and it's really powerful and convenient. Using pprint to print out structure data can make it more clear. You should pay more attention to how we write and use mapper and reducer here. \n",
    "\n",
    "Next, let's load it into Google Cloud and run! Use the following command to load all assignment 5 files into your cluster:\n",
    "~~~~\n",
    "gcloud compute copy-files [LOCAL_FILE_PATH]  [INSTANCE_NAME]:~/\n",
    "~~~~\n",
    "And ssh to your cluster with:\n",
    "~~~~\n",
    "gcloud compute ssh recommend-m-0\n",
    "~~~~\n",
    "Before running the code, we need to install some libraries. start a new terminal via Jupyter Notebook and run the following command in your cluster:\n",
    "~~~~\n",
    "pip install -r requirement.txt\n",
    "~~~~\n",
    "Now you can run your code by \n",
    "~~~~\n",
    "spark-submit --packages org.mongodb.spark:mongo-spark-connector_2.11:2.2.0 \\\n",
    "batch_test.py -mongo MongoDB_URI > output.txt\n",
    "~~~~\n",
    "spark-submit is the shell command how we submit a pyspark job. --packages specify what third-party JAVA packages we will be using. In this case, we use mongodb-spark-connector package. '> output' at the end of the command tells system that print all output into output.txt file. This is a good way to seperate spark log output with what your file print out (otherwise it's a nightmare to find out what your code print out in the massive log printing). After you run the code, you should see something like this in your output.txt file:\n",
    " <img src=\"document/printout.jpeg\" width = \"500\" height = \"700\" alt=\"what\" align=center />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first one is the nice printing of your table_df schema, and the second one is some rows from table_df. Alright, seems you are on your way to create your recommendation system! Also, you don't need to hand in anything for this lab!\n",
    "\n",
    "** quick reference **\n",
    "\n",
    "Here are some addresses for monitoring what happened in your cluster that are really useful:\n",
    "\n",
    "1. http://recommend-m-0:50070 hadoop\n",
    "2. http://recommend-m-0:4040 spark UI\n",
    "3. http://recommend-m-0:8123 jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab2: Write Your First MapReduce and NoSQL\n",
    "In this lab, you will implement [MapReduce](https://en.wikipedia.org/wiki/MapReduce) and [NoSQL](https://en.wikipedia.org/wiki/NoSQL) queries. We provide you a mapreduce class which simulates how spark runs. It will run all your mapreduce job locally and on single machine, there has no parallelism. To start with, let's see a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hadoop is the Elephant King', 'A yellow and elegant thing', 'He never forgets', 'Useful data, or lets', 'An extraneous element cling', 'A wonderful king is Hadoop', 'The elephant plays well with Sqoop', 'But what helps him to thrive', 'Are Impala, and Hive', 'And HDFS in the group']\n",
      "[('A', 2),\n",
      " ('An', 1),\n",
      " ('And', 1),\n",
      " ('Are', 1),\n",
      " ('But', 1),\n",
      " ('Elephant', 1),\n",
      " ('HDFS', 1),\n",
      " ('Hadoop', 2),\n",
      " ('He', 1),\n",
      " ('Hive', 1),\n",
      " ('Impala,', 1),\n",
      " ('King', 1),\n",
      " ('Sqoop', 1),\n",
      " ('The', 1),\n",
      " ('Useful', 1),\n",
      " ('and', 2),\n",
      " ('cling', 1),\n",
      " ('data,', 1),\n",
      " ('elegant', 1),\n",
      " ('element', 1),\n",
      " ('elephant', 1),\n",
      " ('extraneous', 1),\n",
      " ('forgets', 1),\n",
      " ('group', 1),\n",
      " ('helps', 1),\n",
      " ('him', 1),\n",
      " ('in', 1),\n",
      " ('is', 2),\n",
      " ('king', 1),\n",
      " ('lets', 1),\n",
      " ('never', 1),\n",
      " ('or', 1),\n",
      " ('plays', 1),\n",
      " ('the', 2),\n",
      " ('thing', 1),\n",
      " ('thrive', 1),\n",
      " ('to', 1),\n",
      " ('well', 1),\n",
      " ('what', 1),\n",
      " ('with', 1),\n",
      " ('wonderful', 1),\n",
      " ('yellow', 1)]\n"
     ]
    }
   ],
   "source": [
    "from mapreduce import mapreduce\n",
    "from pprint import pprint\n",
    "def mapper1(record):\n",
    "    words = record.split()\n",
    "    return [(word, 1) for word in words]\n",
    "    \n",
    "def reducer(a, b):\n",
    "    return a + b\n",
    "\n",
    "with open('data/hadoop.txt', 'r') as infile:\n",
    "    data = [line.strip() for line in infile]\n",
    "sc = mapreduce()\n",
    "word_count_result = sc.parallelize(data, 4)\\\n",
    "    .flatMap(mapper1)\\\n",
    "    .reduceByKey(reducer)\\\n",
    "    .sortByKey(True)\\\n",
    "    .collect()\n",
    "sc.stop()\n",
    "pprint(list(word_count_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, mapper1 returns a list of tuple, and reducer add all values together to calculate counts for each word. You may also use lambda function to run this mapreduce task.\n",
    "\n",
    "** Task 1 ** inverted_index, which creates an inverted index of a given file. Given a set of documents, an inverted index is a dictionary where each word is associated with a list of the document ids for documents in which that word appears. What you need to do is filling mapper1, reducer, mapper2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['milton-paradise.txt', \"[ Paradise Lost by John Milton 1667 ] Book I Of Man ' s first disobedience , and the fruit Of that forbidden tree whose mortal taste Brought death into the World , and all our woe , With loss of Eden , till one greater Man Restore us , and regain the blissful seat , Sing , Heavenly Muse , that , on the secret top Of Oreb , or of Sinai , didst inspire That shepherd who first taught the chosen seed In the beginning how the heavens and earth Rose out of Chaos : or , if Sion hill Delight thee more , and Siloa ' s brook that flowed Fast by the oracle of God , I thence Invoke thy aid to my adventurous song , That with no middle flight intends to soar Above th ' Aonian mount , while it pursues Things unattempted yet in prose or rhyme .\"], ['edgeworth-parents.txt', \"[ The Parent ' s Assistant , by Maria Edgeworth ] THE ORPHANS . Near the ruins of the castle of Rossmore , in Ireland , is a small cabin , in which there once lived a widow and her four children . As long as she was able to work , she was very industrious , and was accounted the best spinner in the parish ; but she overworked herself at last , and fell ill , so that she could not sit to her wheel as she used to do , and was obliged to give it up to her eldest daughter , Mary .\"], ['austen-emma.txt', \"[ Emma by Jane Austen 1816 ] VOLUME I CHAPTER I Emma Woodhouse , handsome , clever , and rich , with a comfortable home and happy disposition , seemed to unite some of the best blessings of existence ; and had lived nearly twenty - one years in the world with very little to distress or vex her . She was the youngest of the two daughters of a most affectionate , indulgent father ; and had , in consequence of her sister ' s marriage , been mistress of his house from a very early period . Her mother had died too long ago for her to have more than an indistinct remembrance of her caresses ; and her place had been supplied by an excellent woman as governess , who had fallen little short of a mother in affection .\"], ['chesterton-ball.txt', '[ The Ball and The Cross by G . K . Chesterton 1909 ] I . A DISCUSSION SOMEWHAT IN THE AIR The flying ship of Professor Lucifer sang through the skies like a silver arrow ; the bleak white steel of it , gleaming in the bleak blue emptiness of the evening . That it was far above the earth was no expression for it ; to the two men in it , it seemed to be far above the stars . The professor had himself invented the flying machine , and had also invented nearly everything in it .'], ['bible-kjv.txt', '[ The King James Bible ] The Old Testament of the King James Bible The First Book of Moses : Called Genesis 1 : 1 In the beginning God created the heaven and the earth . 1 : 2 And the earth was without form , and void ; and darkness was upon the face of the deep . And the Spirit of God moved upon the face of the waters . 1 : 3 And God said , Let there be light : and there was light . 1 : 4 And God saw the light , that it was good : and God divided the light from the darkness . 1 : 5 And God called the light Day , and the darkness he called Night . And the evening and the morning were the first day .'], ['chesterton-thursday.txt', '[ The Man Who Was Thursday by G . K . Chesterton 1908 ] To Edmund Clerihew Bentley A cloud was on the mind of men , and wailing went the weather , Yea , a sick cloud upon the soul when we were boys together . Science announced nonentity and art admired decay ; The world was old and ended : but you and I were gay ; Round us in antic order their crippled vices came -- Lust that had lost its laughter , fear that had lost its shame . Like the white lock of Whistler , that lit our aimless gloom , Men showed their own white feather as proudly as a plume . Life was a fly that faded , and death a drone that stung ; The world was very old indeed when you and I were young .'], ['blake-poems.txt', '[ Poems by William Blake 1789 ] SONGS OF INNOCENCE AND OF EXPERIENCE and THE BOOK of THEL SONGS OF INNOCENCE INTRODUCTION Piping down the valleys wild , Piping songs of pleasant glee , On a cloud I saw a child , And he laughing said to me : \" Pipe a song about a Lamb !\" So I piped with merry cheer . \" Piper , pipe that song again ;\" So I piped : he wept to hear . \" Drop thy pipe , thy happy pipe ; Sing thy songs of happy cheer :!\" So I sang the same again , While he wept with joy to hear . \" Piper , sit thee down and write In a book , that all may read .\" So he vanish \\' d from my sight ; And I pluck \\' d a hollow reed , And I made a rural pen , And I stain \\' d the water clear , And I wrote my happy songs Every child may joy to hear .'], ['shakespeare-caesar.txt', '[ The Tragedie of Julius Caesar by William Shakespeare 1599 ] Actus Primus . Scoena Prima . Enter Flauius , Murellus , and certaine Commoners ouer the Stage . Flauius . Hence : home you idle Creatures , get you home : Is this a Holiday ? What , know you not ( Being Mechanicall ) you ought not walke Vpon a labouring day , without the signe Of your Profession ? Speake , what Trade art thou ? Car . Why Sir , a Carpenter Mur . Where is thy Leather Apron , and thy Rule ? What dost thou with thy best Apparrell on ? You sir , what Trade are you ? Cobl . Truely Sir , in respect of a fine Workman , I am but as you would say , a Cobler Mur . But what Trade art thou ? Answer me directly Cob . A Trade Sir , that I hope I may vse , with a safe Conscience , which is indeed Sir , a Mender of bad soules Fla .'], ['whitman-leaves.txt', \"[ Leaves of Grass by Walt Whitman 1855 ] Come , said my soul , Such verses for my Body let us write , ( for we are one ,) That should I after return , Or , long , long hence , in other spheres , There to some group of mates the chants resuming , ( Tallying Earth ' s soil , trees , winds , tumultuous waves ,) Ever with pleas ' d smile I may keep on , Ever and ever yet the verses owning -- as , first , I here and now Signing for Soul and Body , set to them my name , Walt Whitman [ BOOK I . INSCRIPTIONS ] } One ' s - Self I Sing One ' s - self I sing , a simple separate person , Yet utter the word Democratic , the word En - Masse .\"], ['melville-moby_dick.txt', '[ Moby Dick by Herman Melville 1851 ] ETYMOLOGY . ( Supplied by a Late Consumptive Usher to a Grammar School ) The pale Usher -- threadbare in coat , heart , body , and brain ; I see him now . He was ever dusting his old lexicons and grammars , with a queer handkerchief , mockingly embellished with all the gay flags of all the known nations of the world . He loved to dust his old grammars ; it somehow mildly reminded him of his mortality .']]\n",
      "[('!\"', ['blake-poems.txt']),\n",
      " ('\"', ['blake-poems.txt']),\n",
      " (\"'\",\n",
      "  ['austen-emma.txt',\n",
      "   'blake-poems.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('(',\n",
      "  ['melville-moby_dick.txt', 'shakespeare-caesar.txt', 'whitman-leaves.txt']),\n",
      " (')', ['melville-moby_dick.txt', 'shakespeare-caesar.txt']),\n",
      " (',',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " (',)', ['whitman-leaves.txt']),\n",
      " ('-', ['austen-emma.txt', 'whitman-leaves.txt']),\n",
      " ('--',\n",
      "  ['chesterton-thursday.txt', 'melville-moby_dick.txt', 'whitman-leaves.txt']),\n",
      " ('.',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('.\"', ['blake-poems.txt']),\n",
      " ('1', ['bible-kjv.txt']),\n",
      " ('1599', ['shakespeare-caesar.txt']),\n",
      " ('1667', ['milton-paradise.txt']),\n",
      " ('1789', ['blake-poems.txt']),\n",
      " ('1816', ['austen-emma.txt']),\n",
      " ('1851', ['melville-moby_dick.txt']),\n",
      " ('1855', ['whitman-leaves.txt']),\n",
      " ('1908', ['chesterton-thursday.txt']),\n",
      " ('1909', ['chesterton-ball.txt']),\n",
      " ('2', ['bible-kjv.txt']),\n",
      " ('3', ['bible-kjv.txt']),\n",
      " ('4', ['bible-kjv.txt']),\n",
      " ('5', ['bible-kjv.txt']),\n",
      " (':',\n",
      "  ['bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt']),\n",
      " (':!\"', ['blake-poems.txt']),\n",
      " (';',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt']),\n",
      " (';\"', ['blake-poems.txt']),\n",
      " ('?', ['shakespeare-caesar.txt']),\n",
      " ('A',\n",
      "  ['chesterton-ball.txt', 'chesterton-thursday.txt', 'shakespeare-caesar.txt']),\n",
      " ('AIR', ['chesterton-ball.txt']),\n",
      " ('AND', ['blake-poems.txt']),\n",
      " ('Above', ['milton-paradise.txt']),\n",
      " ('Actus', ['shakespeare-caesar.txt']),\n",
      " ('And', ['bible-kjv.txt', 'blake-poems.txt']),\n",
      " ('Answer', ['shakespeare-caesar.txt']),\n",
      " ('Aonian', ['milton-paradise.txt']),\n",
      " ('Apparrell', ['shakespeare-caesar.txt']),\n",
      " ('Apron', ['shakespeare-caesar.txt']),\n",
      " ('As', ['edgeworth-parents.txt']),\n",
      " ('Assistant', ['edgeworth-parents.txt']),\n",
      " ('Austen', ['austen-emma.txt']),\n",
      " ('BOOK', ['blake-poems.txt', 'whitman-leaves.txt']),\n",
      " ('Ball', ['chesterton-ball.txt']),\n",
      " ('Being', ['shakespeare-caesar.txt']),\n",
      " ('Bentley', ['chesterton-thursday.txt']),\n",
      " ('Bible', ['bible-kjv.txt']),\n",
      " ('Blake', ['blake-poems.txt']),\n",
      " ('Body', ['whitman-leaves.txt']),\n",
      " ('Book', ['bible-kjv.txt', 'milton-paradise.txt']),\n",
      " ('Brought', ['milton-paradise.txt']),\n",
      " ('But', ['shakespeare-caesar.txt']),\n",
      " ('CHAPTER', ['austen-emma.txt']),\n",
      " ('Caesar', ['shakespeare-caesar.txt']),\n",
      " ('Called', ['bible-kjv.txt']),\n",
      " ('Car', ['shakespeare-caesar.txt']),\n",
      " ('Carpenter', ['shakespeare-caesar.txt']),\n",
      " ('Chaos', ['milton-paradise.txt']),\n",
      " ('Chesterton', ['chesterton-ball.txt', 'chesterton-thursday.txt']),\n",
      " ('Clerihew', ['chesterton-thursday.txt']),\n",
      " ('Cob', ['shakespeare-caesar.txt']),\n",
      " ('Cobl', ['shakespeare-caesar.txt']),\n",
      " ('Cobler', ['shakespeare-caesar.txt']),\n",
      " ('Come', ['whitman-leaves.txt']),\n",
      " ('Commoners', ['shakespeare-caesar.txt']),\n",
      " ('Conscience', ['shakespeare-caesar.txt']),\n",
      " ('Consumptive', ['melville-moby_dick.txt']),\n",
      " ('Creatures', ['shakespeare-caesar.txt']),\n",
      " ('Cross', ['chesterton-ball.txt']),\n",
      " ('DISCUSSION', ['chesterton-ball.txt']),\n",
      " ('Day', ['bible-kjv.txt']),\n",
      " ('Delight', ['milton-paradise.txt']),\n",
      " ('Democratic', ['whitman-leaves.txt']),\n",
      " ('Dick', ['melville-moby_dick.txt']),\n",
      " ('Drop', ['blake-poems.txt']),\n",
      " ('ETYMOLOGY', ['melville-moby_dick.txt']),\n",
      " ('EXPERIENCE', ['blake-poems.txt']),\n",
      " ('Earth', ['whitman-leaves.txt']),\n",
      " ('Eden', ['milton-paradise.txt']),\n",
      " ('Edgeworth', ['edgeworth-parents.txt']),\n",
      " ('Edmund', ['chesterton-thursday.txt']),\n",
      " ('Emma', ['austen-emma.txt']),\n",
      " ('En', ['whitman-leaves.txt']),\n",
      " ('Enter', ['shakespeare-caesar.txt']),\n",
      " ('Ever', ['whitman-leaves.txt']),\n",
      " ('Every', ['blake-poems.txt']),\n",
      " ('Fast', ['milton-paradise.txt']),\n",
      " ('First', ['bible-kjv.txt']),\n",
      " ('Fla', ['shakespeare-caesar.txt']),\n",
      " ('Flauius', ['shakespeare-caesar.txt']),\n",
      " ('G', ['chesterton-ball.txt', 'chesterton-thursday.txt']),\n",
      " ('Genesis', ['bible-kjv.txt']),\n",
      " ('God', ['bible-kjv.txt', 'milton-paradise.txt']),\n",
      " ('Grammar', ['melville-moby_dick.txt']),\n",
      " ('Grass', ['whitman-leaves.txt']),\n",
      " ('He', ['melville-moby_dick.txt']),\n",
      " ('Heavenly', ['milton-paradise.txt']),\n",
      " ('Hence', ['shakespeare-caesar.txt']),\n",
      " ('Her', ['austen-emma.txt']),\n",
      " ('Herman', ['melville-moby_dick.txt']),\n",
      " ('Holiday', ['shakespeare-caesar.txt']),\n",
      " ('I',\n",
      "  ['austen-emma.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('IN', ['chesterton-ball.txt']),\n",
      " ('INNOCENCE', ['blake-poems.txt']),\n",
      " ('INSCRIPTIONS', ['whitman-leaves.txt']),\n",
      " ('INTRODUCTION', ['blake-poems.txt']),\n",
      " ('In', ['bible-kjv.txt', 'blake-poems.txt', 'milton-paradise.txt']),\n",
      " ('Invoke', ['milton-paradise.txt']),\n",
      " ('Ireland', ['edgeworth-parents.txt']),\n",
      " ('Is', ['shakespeare-caesar.txt']),\n",
      " ('James', ['bible-kjv.txt']),\n",
      " ('Jane', ['austen-emma.txt']),\n",
      " ('John', ['milton-paradise.txt']),\n",
      " ('Julius', ['shakespeare-caesar.txt']),\n",
      " ('K', ['chesterton-ball.txt', 'chesterton-thursday.txt']),\n",
      " ('King', ['bible-kjv.txt']),\n",
      " ('Lamb', ['blake-poems.txt']),\n",
      " ('Late', ['melville-moby_dick.txt']),\n",
      " ('Leather', ['shakespeare-caesar.txt']),\n",
      " ('Leaves', ['whitman-leaves.txt']),\n",
      " ('Let', ['bible-kjv.txt']),\n",
      " ('Life', ['chesterton-thursday.txt']),\n",
      " ('Like', ['chesterton-thursday.txt']),\n",
      " ('Lost', ['milton-paradise.txt']),\n",
      " ('Lucifer', ['chesterton-ball.txt']),\n",
      " ('Lust', ['chesterton-thursday.txt']),\n",
      " ('Man', ['chesterton-thursday.txt', 'milton-paradise.txt']),\n",
      " ('Maria', ['edgeworth-parents.txt']),\n",
      " ('Mary', ['edgeworth-parents.txt']),\n",
      " ('Masse', ['whitman-leaves.txt']),\n",
      " ('Mechanicall', ['shakespeare-caesar.txt']),\n",
      " ('Melville', ['melville-moby_dick.txt']),\n",
      " ('Men', ['chesterton-thursday.txt']),\n",
      " ('Mender', ['shakespeare-caesar.txt']),\n",
      " ('Milton', ['milton-paradise.txt']),\n",
      " ('Moby', ['melville-moby_dick.txt']),\n",
      " ('Moses', ['bible-kjv.txt']),\n",
      " ('Mur', ['shakespeare-caesar.txt']),\n",
      " ('Murellus', ['shakespeare-caesar.txt']),\n",
      " ('Muse', ['milton-paradise.txt']),\n",
      " ('Near', ['edgeworth-parents.txt']),\n",
      " ('Night', ['bible-kjv.txt']),\n",
      " ('OF', ['blake-poems.txt']),\n",
      " ('ORPHANS', ['edgeworth-parents.txt']),\n",
      " ('Of', ['milton-paradise.txt', 'shakespeare-caesar.txt']),\n",
      " ('Old', ['bible-kjv.txt']),\n",
      " ('On', ['blake-poems.txt']),\n",
      " ('One', ['whitman-leaves.txt']),\n",
      " ('Or', ['whitman-leaves.txt']),\n",
      " ('Oreb', ['milton-paradise.txt']),\n",
      " ('Paradise', ['milton-paradise.txt']),\n",
      " ('Parent', ['edgeworth-parents.txt']),\n",
      " ('Pipe', ['blake-poems.txt']),\n",
      " ('Piper', ['blake-poems.txt']),\n",
      " ('Piping', ['blake-poems.txt']),\n",
      " ('Poems', ['blake-poems.txt']),\n",
      " ('Prima', ['shakespeare-caesar.txt']),\n",
      " ('Primus', ['shakespeare-caesar.txt']),\n",
      " ('Profession', ['shakespeare-caesar.txt']),\n",
      " ('Professor', ['chesterton-ball.txt']),\n",
      " ('Restore', ['milton-paradise.txt']),\n",
      " ('Rose', ['milton-paradise.txt']),\n",
      " ('Rossmore', ['edgeworth-parents.txt']),\n",
      " ('Round', ['chesterton-thursday.txt']),\n",
      " ('Rule', ['shakespeare-caesar.txt']),\n",
      " ('SOMEWHAT', ['chesterton-ball.txt']),\n",
      " ('SONGS', ['blake-poems.txt']),\n",
      " ('School', ['melville-moby_dick.txt']),\n",
      " ('Science', ['chesterton-thursday.txt']),\n",
      " ('Scoena', ['shakespeare-caesar.txt']),\n",
      " ('Self', ['whitman-leaves.txt']),\n",
      " ('Shakespeare', ['shakespeare-caesar.txt']),\n",
      " ('She', ['austen-emma.txt']),\n",
      " ('Signing', ['whitman-leaves.txt']),\n",
      " ('Siloa', ['milton-paradise.txt']),\n",
      " ('Sinai', ['milton-paradise.txt']),\n",
      " ('Sing', ['blake-poems.txt', 'milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('Sion', ['milton-paradise.txt']),\n",
      " ('Sir', ['shakespeare-caesar.txt']),\n",
      " ('So', ['blake-poems.txt']),\n",
      " ('Soul', ['whitman-leaves.txt']),\n",
      " ('Speake', ['shakespeare-caesar.txt']),\n",
      " ('Spirit', ['bible-kjv.txt']),\n",
      " ('Stage', ['shakespeare-caesar.txt']),\n",
      " ('Such', ['whitman-leaves.txt']),\n",
      " ('Supplied', ['melville-moby_dick.txt']),\n",
      " ('THE', ['blake-poems.txt', 'chesterton-ball.txt', 'edgeworth-parents.txt']),\n",
      " ('THEL', ['blake-poems.txt']),\n",
      " ('Tallying', ['whitman-leaves.txt']),\n",
      " ('Testament', ['bible-kjv.txt']),\n",
      " ('That', ['chesterton-ball.txt', 'milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('The',\n",
      "  ['bible-kjv.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'shakespeare-caesar.txt']),\n",
      " ('There', ['whitman-leaves.txt']),\n",
      " ('Things', ['milton-paradise.txt']),\n",
      " ('Thursday', ['chesterton-thursday.txt']),\n",
      " ('To', ['chesterton-thursday.txt']),\n",
      " ('Trade', ['shakespeare-caesar.txt']),\n",
      " ('Tragedie', ['shakespeare-caesar.txt']),\n",
      " ('Truely', ['shakespeare-caesar.txt']),\n",
      " ('Usher', ['melville-moby_dick.txt']),\n",
      " ('VOLUME', ['austen-emma.txt']),\n",
      " ('Vpon', ['shakespeare-caesar.txt']),\n",
      " ('Walt', ['whitman-leaves.txt']),\n",
      " ('Was', ['chesterton-thursday.txt']),\n",
      " ('What', ['shakespeare-caesar.txt']),\n",
      " ('Where', ['shakespeare-caesar.txt']),\n",
      " ('While', ['blake-poems.txt']),\n",
      " ('Whistler', ['chesterton-thursday.txt']),\n",
      " ('Whitman', ['whitman-leaves.txt']),\n",
      " ('Who', ['chesterton-thursday.txt']),\n",
      " ('Why', ['shakespeare-caesar.txt']),\n",
      " ('William', ['blake-poems.txt', 'shakespeare-caesar.txt']),\n",
      " ('With', ['milton-paradise.txt']),\n",
      " ('Woodhouse', ['austen-emma.txt']),\n",
      " ('Workman', ['shakespeare-caesar.txt']),\n",
      " ('World', ['milton-paradise.txt']),\n",
      " ('Yea', ['chesterton-thursday.txt']),\n",
      " ('Yet', ['whitman-leaves.txt']),\n",
      " ('You', ['shakespeare-caesar.txt']),\n",
      " ('[',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " (']',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('a',\n",
      "  ['austen-emma.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('able', ['edgeworth-parents.txt']),\n",
      " ('about', ['blake-poems.txt']),\n",
      " ('above', ['chesterton-ball.txt']),\n",
      " ('accounted', ['edgeworth-parents.txt']),\n",
      " ('admired', ['chesterton-thursday.txt']),\n",
      " ('adventurous', ['milton-paradise.txt']),\n",
      " ('affection', ['austen-emma.txt']),\n",
      " ('affectionate', ['austen-emma.txt']),\n",
      " ('after', ['whitman-leaves.txt']),\n",
      " ('again', ['blake-poems.txt']),\n",
      " ('ago', ['austen-emma.txt']),\n",
      " ('aid', ['milton-paradise.txt']),\n",
      " ('aimless', ['chesterton-thursday.txt']),\n",
      " ('all', ['blake-poems.txt', 'melville-moby_dick.txt', 'milton-paradise.txt']),\n",
      " ('also', ['chesterton-ball.txt']),\n",
      " ('am', ['shakespeare-caesar.txt']),\n",
      " ('an', ['austen-emma.txt']),\n",
      " ('and',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('announced', ['chesterton-thursday.txt']),\n",
      " ('antic', ['chesterton-thursday.txt']),\n",
      " ('are', ['shakespeare-caesar.txt', 'whitman-leaves.txt']),\n",
      " ('arrow', ['chesterton-ball.txt']),\n",
      " ('art', ['chesterton-thursday.txt', 'shakespeare-caesar.txt']),\n",
      " ('as',\n",
      "  ['austen-emma.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('at', ['edgeworth-parents.txt']),\n",
      " ('bad', ['shakespeare-caesar.txt']),\n",
      " ('be', ['bible-kjv.txt', 'chesterton-ball.txt']),\n",
      " ('been', ['austen-emma.txt']),\n",
      " ('beginning', ['bible-kjv.txt', 'milton-paradise.txt']),\n",
      " ('best',\n",
      "  ['austen-emma.txt', 'edgeworth-parents.txt', 'shakespeare-caesar.txt']),\n",
      " ('bleak', ['chesterton-ball.txt']),\n",
      " ('blessings', ['austen-emma.txt']),\n",
      " ('blissful', ['milton-paradise.txt']),\n",
      " ('blue', ['chesterton-ball.txt']),\n",
      " ('body', ['melville-moby_dick.txt']),\n",
      " ('book', ['blake-poems.txt']),\n",
      " ('boys', ['chesterton-thursday.txt']),\n",
      " ('brain', ['melville-moby_dick.txt']),\n",
      " ('brook', ['milton-paradise.txt']),\n",
      " ('but',\n",
      "  ['chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'shakespeare-caesar.txt']),\n",
      " ('by',\n",
      "  ['austen-emma.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('cabin', ['edgeworth-parents.txt']),\n",
      " ('called', ['bible-kjv.txt']),\n",
      " ('came', ['chesterton-thursday.txt']),\n",
      " ('caresses', ['austen-emma.txt']),\n",
      " ('castle', ['edgeworth-parents.txt']),\n",
      " ('certaine', ['shakespeare-caesar.txt']),\n",
      " ('chants', ['whitman-leaves.txt']),\n",
      " ('cheer', ['blake-poems.txt']),\n",
      " ('child', ['blake-poems.txt']),\n",
      " ('children', ['edgeworth-parents.txt']),\n",
      " ('chosen', ['milton-paradise.txt']),\n",
      " ('clear', ['blake-poems.txt']),\n",
      " ('clever', ['austen-emma.txt']),\n",
      " ('cloud', ['blake-poems.txt', 'chesterton-thursday.txt']),\n",
      " ('coat', ['melville-moby_dick.txt']),\n",
      " ('comfortable', ['austen-emma.txt']),\n",
      " ('consequence', ['austen-emma.txt']),\n",
      " ('could', ['edgeworth-parents.txt']),\n",
      " ('created', ['bible-kjv.txt']),\n",
      " ('crippled', ['chesterton-thursday.txt']),\n",
      " ('d', ['blake-poems.txt', 'whitman-leaves.txt']),\n",
      " ('darkness', ['bible-kjv.txt']),\n",
      " ('daughter', ['edgeworth-parents.txt']),\n",
      " ('daughters', ['austen-emma.txt']),\n",
      " ('day', ['bible-kjv.txt', 'shakespeare-caesar.txt']),\n",
      " ('death', ['chesterton-thursday.txt', 'milton-paradise.txt']),\n",
      " ('decay', ['chesterton-thursday.txt']),\n",
      " ('deep', ['bible-kjv.txt']),\n",
      " ('didst', ['milton-paradise.txt']),\n",
      " ('died', ['austen-emma.txt']),\n",
      " ('directly', ['shakespeare-caesar.txt']),\n",
      " ('disobedience', ['milton-paradise.txt']),\n",
      " ('disposition', ['austen-emma.txt']),\n",
      " ('distress', ['austen-emma.txt']),\n",
      " ('divided', ['bible-kjv.txt']),\n",
      " ('do', ['edgeworth-parents.txt']),\n",
      " ('dost', ['shakespeare-caesar.txt']),\n",
      " ('down', ['blake-poems.txt']),\n",
      " ('drone', ['chesterton-thursday.txt']),\n",
      " ('dust', ['melville-moby_dick.txt']),\n",
      " ('dusting', ['melville-moby_dick.txt']),\n",
      " ('early', ['austen-emma.txt']),\n",
      " ('earth', ['bible-kjv.txt', 'chesterton-ball.txt', 'milton-paradise.txt']),\n",
      " ('eldest', ['edgeworth-parents.txt']),\n",
      " ('embellished', ['melville-moby_dick.txt']),\n",
      " ('emptiness', ['chesterton-ball.txt']),\n",
      " ('ended', ['chesterton-thursday.txt']),\n",
      " ('evening', ['bible-kjv.txt', 'chesterton-ball.txt']),\n",
      " ('ever', ['melville-moby_dick.txt', 'whitman-leaves.txt']),\n",
      " ('everything', ['chesterton-ball.txt']),\n",
      " ('excellent', ['austen-emma.txt']),\n",
      " ('existence', ['austen-emma.txt']),\n",
      " ('expression', ['chesterton-ball.txt']),\n",
      " ('face', ['bible-kjv.txt']),\n",
      " ('faded', ['chesterton-thursday.txt']),\n",
      " ('fallen', ['austen-emma.txt']),\n",
      " ('far', ['chesterton-ball.txt']),\n",
      " ('father', ['austen-emma.txt']),\n",
      " ('fear', ['chesterton-thursday.txt']),\n",
      " ('feather', ['chesterton-thursday.txt']),\n",
      " ('fell', ['edgeworth-parents.txt']),\n",
      " ('fine', ['shakespeare-caesar.txt']),\n",
      " ('first', ['bible-kjv.txt', 'milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('flags', ['melville-moby_dick.txt']),\n",
      " ('flight', ['milton-paradise.txt']),\n",
      " ('flowed', ['milton-paradise.txt']),\n",
      " ('fly', ['chesterton-thursday.txt']),\n",
      " ('flying', ['chesterton-ball.txt']),\n",
      " ('for', ['austen-emma.txt', 'chesterton-ball.txt', 'whitman-leaves.txt']),\n",
      " ('forbidden', ['milton-paradise.txt']),\n",
      " ('form', ['bible-kjv.txt']),\n",
      " ('four', ['edgeworth-parents.txt']),\n",
      " ('from', ['austen-emma.txt', 'bible-kjv.txt', 'blake-poems.txt']),\n",
      " ('fruit', ['milton-paradise.txt']),\n",
      " ('gay', ['chesterton-thursday.txt', 'melville-moby_dick.txt']),\n",
      " ('get', ['shakespeare-caesar.txt']),\n",
      " ('give', ['edgeworth-parents.txt']),\n",
      " ('gleaming', ['chesterton-ball.txt']),\n",
      " ('glee', ['blake-poems.txt']),\n",
      " ('gloom', ['chesterton-thursday.txt']),\n",
      " ('good', ['bible-kjv.txt']),\n",
      " ('governess', ['austen-emma.txt']),\n",
      " ('grammars', ['melville-moby_dick.txt']),\n",
      " ('greater', ['milton-paradise.txt']),\n",
      " ('group', ['whitman-leaves.txt']),\n",
      " ('had', ['austen-emma.txt', 'chesterton-ball.txt', 'chesterton-thursday.txt']),\n",
      " ('handkerchief', ['melville-moby_dick.txt']),\n",
      " ('handsome', ['austen-emma.txt']),\n",
      " ('happy', ['austen-emma.txt', 'blake-poems.txt']),\n",
      " ('have', ['austen-emma.txt']),\n",
      " ('he', ['bible-kjv.txt', 'blake-poems.txt']),\n",
      " ('hear', ['blake-poems.txt']),\n",
      " ('heart', ['melville-moby_dick.txt']),\n",
      " ('heaven', ['bible-kjv.txt']),\n",
      " ('heavens', ['milton-paradise.txt']),\n",
      " ('hence', ['whitman-leaves.txt']),\n",
      " ('her', ['austen-emma.txt', 'edgeworth-parents.txt']),\n",
      " ('here', ['whitman-leaves.txt']),\n",
      " ('herself', ['edgeworth-parents.txt']),\n",
      " ('hill', ['milton-paradise.txt']),\n",
      " ('him', ['melville-moby_dick.txt']),\n",
      " ('himself', ['chesterton-ball.txt']),\n",
      " ('his', ['austen-emma.txt', 'melville-moby_dick.txt']),\n",
      " ('hollow', ['blake-poems.txt']),\n",
      " ('home', ['austen-emma.txt', 'shakespeare-caesar.txt']),\n",
      " ('hope', ['shakespeare-caesar.txt']),\n",
      " ('house', ['austen-emma.txt']),\n",
      " ('how', ['milton-paradise.txt']),\n",
      " ('idle', ['shakespeare-caesar.txt']),\n",
      " ('if', ['milton-paradise.txt']),\n",
      " ('ill', ['edgeworth-parents.txt']),\n",
      " ('in',\n",
      "  ['austen-emma.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('indeed', ['chesterton-thursday.txt', 'shakespeare-caesar.txt']),\n",
      " ('indistinct', ['austen-emma.txt']),\n",
      " ('indulgent', ['austen-emma.txt']),\n",
      " ('industrious', ['edgeworth-parents.txt']),\n",
      " ('inspire', ['milton-paradise.txt']),\n",
      " ('intends', ['milton-paradise.txt']),\n",
      " ('into', ['milton-paradise.txt']),\n",
      " ('invented', ['chesterton-ball.txt']),\n",
      " ('is', ['edgeworth-parents.txt', 'shakespeare-caesar.txt']),\n",
      " ('it',\n",
      "  ['bible-kjv.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt']),\n",
      " ('its', ['chesterton-thursday.txt']),\n",
      " ('joy', ['blake-poems.txt']),\n",
      " ('keep', ['whitman-leaves.txt']),\n",
      " ('know', ['shakespeare-caesar.txt']),\n",
      " ('known', ['melville-moby_dick.txt']),\n",
      " ('labouring', ['shakespeare-caesar.txt']),\n",
      " ('last', ['edgeworth-parents.txt']),\n",
      " ('laughing', ['blake-poems.txt']),\n",
      " ('laughter', ['chesterton-thursday.txt']),\n",
      " ('let', ['whitman-leaves.txt']),\n",
      " ('lexicons', ['melville-moby_dick.txt']),\n",
      " ('light', ['bible-kjv.txt']),\n",
      " ('like', ['chesterton-ball.txt']),\n",
      " ('lit', ['chesterton-thursday.txt']),\n",
      " ('little', ['austen-emma.txt']),\n",
      " ('lived', ['austen-emma.txt', 'edgeworth-parents.txt']),\n",
      " ('lock', ['chesterton-thursday.txt']),\n",
      " ('long', ['austen-emma.txt', 'edgeworth-parents.txt', 'whitman-leaves.txt']),\n",
      " ('loss', ['milton-paradise.txt']),\n",
      " ('lost', ['chesterton-thursday.txt']),\n",
      " ('loved', ['melville-moby_dick.txt']),\n",
      " ('machine', ['chesterton-ball.txt']),\n",
      " ('made', ['blake-poems.txt']),\n",
      " ('marriage', ['austen-emma.txt']),\n",
      " ('mates', ['whitman-leaves.txt']),\n",
      " ('may', ['blake-poems.txt', 'shakespeare-caesar.txt', 'whitman-leaves.txt']),\n",
      " ('me', ['blake-poems.txt', 'shakespeare-caesar.txt']),\n",
      " ('men', ['chesterton-ball.txt', 'chesterton-thursday.txt']),\n",
      " ('merry', ['blake-poems.txt']),\n",
      " ('middle', ['milton-paradise.txt']),\n",
      " ('mildly', ['melville-moby_dick.txt']),\n",
      " ('mind', ['chesterton-thursday.txt']),\n",
      " ('mistress', ['austen-emma.txt']),\n",
      " ('mockingly', ['melville-moby_dick.txt']),\n",
      " ('more', ['austen-emma.txt', 'milton-paradise.txt']),\n",
      " ('morning', ['bible-kjv.txt']),\n",
      " ('mortal', ['milton-paradise.txt']),\n",
      " ('mortality', ['melville-moby_dick.txt']),\n",
      " ('most', ['austen-emma.txt']),\n",
      " ('mother', ['austen-emma.txt']),\n",
      " ('mount', ['milton-paradise.txt']),\n",
      " ('moved', ['bible-kjv.txt']),\n",
      " ('my', ['blake-poems.txt', 'milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('name', ['whitman-leaves.txt']),\n",
      " ('nations', ['melville-moby_dick.txt']),\n",
      " ('nearly', ['austen-emma.txt', 'chesterton-ball.txt']),\n",
      " ('no', ['chesterton-ball.txt', 'milton-paradise.txt']),\n",
      " ('nonentity', ['chesterton-thursday.txt']),\n",
      " ('not', ['edgeworth-parents.txt', 'shakespeare-caesar.txt']),\n",
      " ('now', ['melville-moby_dick.txt', 'whitman-leaves.txt']),\n",
      " ('obliged', ['edgeworth-parents.txt']),\n",
      " ('of',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('old', ['chesterton-thursday.txt', 'melville-moby_dick.txt']),\n",
      " ('on',\n",
      "  ['chesterton-thursday.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('once', ['edgeworth-parents.txt']),\n",
      " ('one', ['austen-emma.txt', 'milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('or', ['austen-emma.txt', 'milton-paradise.txt']),\n",
      " ('oracle', ['milton-paradise.txt']),\n",
      " ('order', ['chesterton-thursday.txt']),\n",
      " ('other', ['whitman-leaves.txt']),\n",
      " ('ouer', ['shakespeare-caesar.txt']),\n",
      " ('ought', ['shakespeare-caesar.txt']),\n",
      " ('our', ['chesterton-thursday.txt', 'milton-paradise.txt']),\n",
      " ('out', ['milton-paradise.txt']),\n",
      " ('overworked', ['edgeworth-parents.txt']),\n",
      " ('own', ['chesterton-thursday.txt']),\n",
      " ('owning', ['whitman-leaves.txt']),\n",
      " ('pale', ['melville-moby_dick.txt']),\n",
      " ('parish', ['edgeworth-parents.txt']),\n",
      " ('pen', ['blake-poems.txt']),\n",
      " ('period', ['austen-emma.txt']),\n",
      " ('person', ['whitman-leaves.txt']),\n",
      " ('pipe', ['blake-poems.txt']),\n",
      " ('piped', ['blake-poems.txt']),\n",
      " ('place', ['austen-emma.txt']),\n",
      " ('pleas', ['whitman-leaves.txt']),\n",
      " ('pleasant', ['blake-poems.txt']),\n",
      " ('pluck', ['blake-poems.txt']),\n",
      " ('plume', ['chesterton-thursday.txt']),\n",
      " ('professor', ['chesterton-ball.txt']),\n",
      " ('prose', ['milton-paradise.txt']),\n",
      " ('proudly', ['chesterton-thursday.txt']),\n",
      " ('pursues', ['milton-paradise.txt']),\n",
      " ('queer', ['melville-moby_dick.txt']),\n",
      " ('read', ['blake-poems.txt']),\n",
      " ('reed', ['blake-poems.txt']),\n",
      " ('regain', ['milton-paradise.txt']),\n",
      " ('remembrance', ['austen-emma.txt']),\n",
      " ('reminded', ['melville-moby_dick.txt']),\n",
      " ('respect', ['shakespeare-caesar.txt']),\n",
      " ('resuming', ['whitman-leaves.txt']),\n",
      " ('return', ['whitman-leaves.txt']),\n",
      " ('rhyme', ['milton-paradise.txt']),\n",
      " ('rich', ['austen-emma.txt']),\n",
      " ('ruins', ['edgeworth-parents.txt']),\n",
      " ('rural', ['blake-poems.txt']),\n",
      " ('s',\n",
      "  ['austen-emma.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('safe', ['shakespeare-caesar.txt']),\n",
      " ('said', ['bible-kjv.txt', 'blake-poems.txt', 'whitman-leaves.txt']),\n",
      " ('same', ['blake-poems.txt']),\n",
      " ('sang', ['blake-poems.txt', 'chesterton-ball.txt']),\n",
      " ('saw', ['bible-kjv.txt', 'blake-poems.txt']),\n",
      " ('say', ['shakespeare-caesar.txt']),\n",
      " ('seat', ['milton-paradise.txt']),\n",
      " ('secret', ['milton-paradise.txt']),\n",
      " ('see', ['melville-moby_dick.txt']),\n",
      " ('seed', ['milton-paradise.txt']),\n",
      " ('seemed', ['austen-emma.txt', 'chesterton-ball.txt']),\n",
      " ('self', ['whitman-leaves.txt']),\n",
      " ('separate', ['whitman-leaves.txt']),\n",
      " ('set', ['whitman-leaves.txt']),\n",
      " ('shame', ['chesterton-thursday.txt']),\n",
      " ('she', ['edgeworth-parents.txt']),\n",
      " ('shepherd', ['milton-paradise.txt']),\n",
      " ('ship', ['chesterton-ball.txt']),\n",
      " ('short', ['austen-emma.txt']),\n",
      " ('should', ['whitman-leaves.txt']),\n",
      " ('showed', ['chesterton-thursday.txt']),\n",
      " ('sick', ['chesterton-thursday.txt']),\n",
      " ('sight', ['blake-poems.txt']),\n",
      " ('signe', ['shakespeare-caesar.txt']),\n",
      " ('silver', ['chesterton-ball.txt']),\n",
      " ('simple', ['whitman-leaves.txt']),\n",
      " ('sing', ['whitman-leaves.txt']),\n",
      " ('sir', ['shakespeare-caesar.txt']),\n",
      " ('sister', ['austen-emma.txt']),\n",
      " ('sit', ['blake-poems.txt', 'edgeworth-parents.txt']),\n",
      " ('skies', ['chesterton-ball.txt']),\n",
      " ('small', ['edgeworth-parents.txt']),\n",
      " ('smile', ['whitman-leaves.txt']),\n",
      " ('so', ['edgeworth-parents.txt']),\n",
      " ('soar', ['milton-paradise.txt']),\n",
      " ('soil', ['whitman-leaves.txt']),\n",
      " ('some', ['austen-emma.txt', 'whitman-leaves.txt']),\n",
      " ('somehow', ['melville-moby_dick.txt']),\n",
      " ('song', ['blake-poems.txt', 'milton-paradise.txt']),\n",
      " ('songs', ['blake-poems.txt']),\n",
      " ('soul', ['chesterton-thursday.txt', 'whitman-leaves.txt']),\n",
      " ('soules', ['shakespeare-caesar.txt']),\n",
      " ('spheres', ['whitman-leaves.txt']),\n",
      " ('spinner', ['edgeworth-parents.txt']),\n",
      " ('stain', ['blake-poems.txt']),\n",
      " ('stars', ['chesterton-ball.txt']),\n",
      " ('steel', ['chesterton-ball.txt']),\n",
      " ('stung', ['chesterton-thursday.txt']),\n",
      " ('supplied', ['austen-emma.txt']),\n",
      " ('taste', ['milton-paradise.txt']),\n",
      " ('taught', ['milton-paradise.txt']),\n",
      " ('th', ['milton-paradise.txt']),\n",
      " ('than', ['austen-emma.txt']),\n",
      " ('that',\n",
      "  ['bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt']),\n",
      " ('the',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('thee', ['blake-poems.txt', 'milton-paradise.txt']),\n",
      " ('their', ['chesterton-thursday.txt']),\n",
      " ('them', ['whitman-leaves.txt']),\n",
      " ('thence', ['milton-paradise.txt']),\n",
      " ('there', ['bible-kjv.txt', 'edgeworth-parents.txt']),\n",
      " ('this', ['shakespeare-caesar.txt']),\n",
      " ('thou', ['shakespeare-caesar.txt']),\n",
      " ('threadbare', ['melville-moby_dick.txt']),\n",
      " ('through', ['chesterton-ball.txt']),\n",
      " ('thy', ['blake-poems.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt']),\n",
      " ('till', ['milton-paradise.txt']),\n",
      " ('to',\n",
      "  ['austen-emma.txt',\n",
      "   'blake-poems.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('together', ['chesterton-thursday.txt']),\n",
      " ('too', ['austen-emma.txt']),\n",
      " ('top', ['milton-paradise.txt']),\n",
      " ('tree', ['milton-paradise.txt']),\n",
      " ('trees', ['whitman-leaves.txt']),\n",
      " ('tumultuous', ['whitman-leaves.txt']),\n",
      " ('twenty', ['austen-emma.txt']),\n",
      " ('two', ['austen-emma.txt', 'chesterton-ball.txt']),\n",
      " ('unattempted', ['milton-paradise.txt']),\n",
      " ('unite', ['austen-emma.txt']),\n",
      " ('up', ['edgeworth-parents.txt']),\n",
      " ('upon', ['bible-kjv.txt', 'chesterton-thursday.txt']),\n",
      " ('us',\n",
      "  ['chesterton-thursday.txt', 'milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('used', ['edgeworth-parents.txt']),\n",
      " ('utter', ['whitman-leaves.txt']),\n",
      " ('valleys', ['blake-poems.txt']),\n",
      " ('vanish', ['blake-poems.txt']),\n",
      " ('verses', ['whitman-leaves.txt']),\n",
      " ('very',\n",
      "  ['austen-emma.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt']),\n",
      " ('vex', ['austen-emma.txt']),\n",
      " ('vices', ['chesterton-thursday.txt']),\n",
      " ('void', ['bible-kjv.txt']),\n",
      " ('vse', ['shakespeare-caesar.txt']),\n",
      " ('wailing', ['chesterton-thursday.txt']),\n",
      " ('walke', ['shakespeare-caesar.txt']),\n",
      " ('was',\n",
      "  ['austen-emma.txt',\n",
      "   'bible-kjv.txt',\n",
      "   'chesterton-ball.txt',\n",
      "   'chesterton-thursday.txt',\n",
      "   'edgeworth-parents.txt',\n",
      "   'melville-moby_dick.txt']),\n",
      " ('water', ['blake-poems.txt']),\n",
      " ('waters', ['bible-kjv.txt']),\n",
      " ('waves', ['whitman-leaves.txt']),\n",
      " ('we', ['chesterton-thursday.txt', 'whitman-leaves.txt']),\n",
      " ('weather', ['chesterton-thursday.txt']),\n",
      " ('went', ['chesterton-thursday.txt']),\n",
      " ('wept', ['blake-poems.txt']),\n",
      " ('were', ['bible-kjv.txt', 'chesterton-thursday.txt']),\n",
      " ('what', ['shakespeare-caesar.txt']),\n",
      " ('wheel', ['edgeworth-parents.txt']),\n",
      " ('when', ['chesterton-thursday.txt']),\n",
      " ('which', ['edgeworth-parents.txt', 'shakespeare-caesar.txt']),\n",
      " ('while', ['milton-paradise.txt']),\n",
      " ('white', ['chesterton-ball.txt', 'chesterton-thursday.txt']),\n",
      " ('who', ['austen-emma.txt', 'milton-paradise.txt']),\n",
      " ('whose', ['milton-paradise.txt']),\n",
      " ('widow', ['edgeworth-parents.txt']),\n",
      " ('wild', ['blake-poems.txt']),\n",
      " ('winds', ['whitman-leaves.txt']),\n",
      " ('with',\n",
      "  ['austen-emma.txt',\n",
      "   'blake-poems.txt',\n",
      "   'melville-moby_dick.txt',\n",
      "   'milton-paradise.txt',\n",
      "   'shakespeare-caesar.txt',\n",
      "   'whitman-leaves.txt']),\n",
      " ('without', ['bible-kjv.txt', 'shakespeare-caesar.txt']),\n",
      " ('woe', ['milton-paradise.txt']),\n",
      " ('woman', ['austen-emma.txt']),\n",
      " ('word', ['whitman-leaves.txt']),\n",
      " ('work', ['edgeworth-parents.txt']),\n",
      " ('world',\n",
      "  ['austen-emma.txt', 'chesterton-thursday.txt', 'melville-moby_dick.txt']),\n",
      " ('would', ['shakespeare-caesar.txt']),\n",
      " ('write', ['blake-poems.txt', 'whitman-leaves.txt']),\n",
      " ('wrote', ['blake-poems.txt']),\n",
      " ('years', ['austen-emma.txt']),\n",
      " ('yet', ['milton-paradise.txt', 'whitman-leaves.txt']),\n",
      " ('you', ['chesterton-thursday.txt', 'shakespeare-caesar.txt']),\n",
      " ('young', ['chesterton-thursday.txt']),\n",
      " ('youngest', ['austen-emma.txt']),\n",
      " ('your', ['shakespeare-caesar.txt']),\n",
      " ('}', ['whitman-leaves.txt'])]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "def mapper1(record):\n",
    "    \"\"\"\n",
    "    The document text may have words in various cases (i.e., upper an lowercase, or mixed) or elements of punctuation.\n",
    "    Do not modify the string, and treat each token as if it was a valid word. (That is, just use value.split())\n",
    "    :param record: [document_id, text]\n",
    "                    document_id: document identifier formatted as a string\n",
    "                    text: text of the document formatted as a string\n",
    "    :return:       [(word, document_id), (word, document_id), ...]\n",
    "    \"\"\"\n",
    "    tmpresult = []\n",
    "    tmptext = record[1].split()\n",
    "    for item in tmptext:\n",
    "        tmpresult.append((item,[record[0]]))\n",
    "    return tmpresult\n",
    "\n",
    "# a: a group of document_ids\n",
    "# b: a group of document_ids\n",
    "def reducer(a, b):\n",
    "    \"\"\"\n",
    "    The input variables, a and b, are each a group of document ids. You will want to join them and return that result.\n",
    "    The group may consist of one or multiple documents, make sure that your code works for both cases.\n",
    "    :param a: a group of document_ids\n",
    "           b: a group of document_ids\n",
    "    :return: joined document_id list\n",
    "    Hint: in Mapper 1, you may want to wrap the document_id in a list, making joining groups of them easier \n",
    "    (especially as it simplifies the cases above!). It is easiest to let a group of document_ids be a list of document_ids.\n",
    "    \"\"\"\n",
    "    return a+b\n",
    "\n",
    "# record: \n",
    "def mapper2(record):\n",
    "    \"\"\"\n",
    "    :param record: (word, [document_id1, document_id2, ...])\n",
    "                    word: a word\n",
    "                    [document_id1, document_id2, ...]: unsorted list of document id's\n",
    "    return (word, sorted list of unique document id's)\n",
    "    \"\"\"\n",
    "\n",
    "    tmp = list(set(record[1]))\n",
    "    tmp.sort()\n",
    "    return (record[0], tmp)\n",
    "\n",
    "# Do not modify\n",
    "with open('data/books.json', 'r') as infile:\n",
    "    data = [json.loads(line) for line in infile]\n",
    "\n",
    "sc = mapreduce()\n",
    "inverted_index_result = sc.parallelize(data, 128) \\\n",
    "    .flatMap(mapper1) \\\n",
    "    .reduceByKey(reducer) \\\n",
    "    .sortByKey(True) \\\n",
    "    .map(mapper2) \\\n",
    "    .collect()\n",
    "\n",
    "sc.stop()\n",
    "pprint(inverted_index_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Task 2 ** matrix multiplication, which calculate multiplication of two sparse matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0, 11878),\n",
      " (0, 1, 14044),\n",
      " (0, 2, 16031),\n",
      " (0, 3, 5964),\n",
      " (0, 4, 15874),\n",
      " (1, 0, 4081),\n",
      " (1, 1, 6914),\n",
      " (1, 2, 8282),\n",
      " (1, 3, 7479),\n",
      " (1, 4, 9647),\n",
      " (2, 0, 6844),\n",
      " (2, 1, 9880),\n",
      " (2, 2, 10636),\n",
      " (2, 3, 6973),\n",
      " (2, 4, 8873),\n",
      " (3, 0, 10512),\n",
      " (3, 1, 12037),\n",
      " (3, 2, 10587),\n",
      " (3, 3, 2934),\n",
      " (3, 4, 5274),\n",
      " (4, 0, 11182),\n",
      " (4, 1, 14591),\n",
      " (4, 2, 10954),\n",
      " (4, 3, 1660),\n",
      " (4, 4, 9981)]\n"
     ]
    }
   ],
   "source": [
    "i_range = 5\n",
    "j_range = 5\n",
    "\n",
    "def mapper1(record):\n",
    "    \"\"\"\n",
    "    :param record: (matrix, i, j, value)\n",
    "                    matrix: which matrix this record belong to\n",
    "                    i: row number\n",
    "                    j: column number\n",
    "                    value: value\n",
    "    return:        [((k, f), record)]\n",
    "                    k: the corresponding new matrix row indexs\n",
    "                    f: the corresponding new matrix column indexs\n",
    "                    record: input record\n",
    "    \"\"\"\n",
    "    global i_range\n",
    "    global j_range\n",
    "    result = []\n",
    "    if record[0]=='a':\n",
    "        for j in range(0, j_range):\n",
    "            tmp = ((record[1], j), [record])\n",
    "            result.append(tmp)\n",
    "    if record[0]=='b':\n",
    "        for i in range(0, i_range):\n",
    "            tmp = ((i, record[2]), [record])\n",
    "            result.append(tmp)\n",
    "    #return [result[0], result[1], result[2], result[3], result[4]]\n",
    "    return result\n",
    "\n",
    "# same as task 1\n",
    "def reducer(a, b):\n",
    "    return a+b\n",
    "\n",
    "def mapper2(record):\n",
    "    \"\"\"\n",
    "    :param record: ((k, f), [(matrix, i, j, value),...])\n",
    "    :return         (k, f, value)\n",
    "                    value: new matrix value at position (k, f)\n",
    "    \"\"\"\n",
    "    lista = []\n",
    "    listb = []\n",
    "    value = 0\n",
    "    for item in record[1]:\n",
    "        lista.append(item) if item[0]=='a' else listb.append(item)\n",
    "    for itema in lista:\n",
    "        for itemb in listb:\n",
    "            if (itema[2]==itemb[1]):\n",
    "                value += itema[3]*itemb[3]\n",
    "    return (record[0][0], record[0][1], value)\n",
    "\n",
    "\n",
    "with open('data/matrix.json', 'r') as infile:\n",
    "    data = [json.loads(line) for line in infile]\n",
    "\n",
    "sc = mapreduce()\n",
    "multiply_result = sc.parallelize(data, 128) \\\n",
    "    .flatMap(mapper1) \\\n",
    "    .reduceByKey(reducer) \\\n",
    "    .map(mapper2) \\\n",
    "    .collect()\n",
    "sc.stop()\n",
    "pprint(multiply_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "***\n",
    "That's all for MapReduce, let's now move onto NoSQL! We choose to use MongoDB as our NoSQL database in this assignment. MongoDB provides a tool named Mongo Shell to interact with it. However, it requires you to install mongodb locally. Python provides a library named pymongo to perform the same function, and we will use it in this lab(again you see how greate to use Python, it's the glue for everything).\n",
    "\n",
    "First of all, we need to connect to your mongodb. Fill in your database URI in MongoClient bracket.\n",
    "\n",
    "** note **\n",
    "try to use methods that doesn't raise deprecateWarning. Also, keep in mind that find function returns a cursor and you need to for loop it and print out query results.\n",
    "\n",
    "First, we need to load data into memory, to do so, you need to run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from pymongo import *\n",
    "\n",
    "client = MongoClient(\"mongodb://jinyan:12345678@ds151973.mlab.com:51973/netflix\")\n",
    "db = client.netflix\n",
    "cl = db.watch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we can directly use .database_name and .collection_name to access corresponding records. All of NoSQL operator can be found on [pymongo website](https://api.mongodb.com/python/current/), you can use any query operators as you like to finish the task. remember to use pprint to print your result out for each NoSQL.\n",
    "\n",
    "NoSQL 1: find document whose movie_id 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': ObjectId('59a2c83ff787c87416ca577d'),\n",
      " 'movie_id': 4993,\n",
      " 'user_list': [8,\n",
      "               13,\n",
      "               15,\n",
      "               17,\n",
      "               20,\n",
      "               22,\n",
      "               23,\n",
      "               30,\n",
      "               31,\n",
      "               38,\n",
      "               40,\n",
      "               42,\n",
      "               46,\n",
      "               48,\n",
      "               56,\n",
      "               59,\n",
      "               61,\n",
      "               63,\n",
      "               68,\n",
      "               69,\n",
      "               72,\n",
      "               73,\n",
      "               75,\n",
      "               77,\n",
      "               78,\n",
      "               79,\n",
      "               84,\n",
      "               88,\n",
      "               91,\n",
      "               93,\n",
      "               94,\n",
      "               95,\n",
      "               99,\n",
      "               101,\n",
      "               104,\n",
      "               105,\n",
      "               109,\n",
      "               111,\n",
      "               116,\n",
      "               124,\n",
      "               125,\n",
      "               128,\n",
      "               130,\n",
      "               133,\n",
      "               134,\n",
      "               136,\n",
      "               138,\n",
      "               148,\n",
      "               150,\n",
      "               152,\n",
      "               157,\n",
      "               159,\n",
      "               163,\n",
      "               164,\n",
      "               169,\n",
      "               175,\n",
      "               176,\n",
      "               185,\n",
      "               186,\n",
      "               187,\n",
      "               199,\n",
      "               201,\n",
      "               205,\n",
      "               212,\n",
      "               213,\n",
      "               216,\n",
      "               219,\n",
      "               228,\n",
      "               234,\n",
      "               236,\n",
      "               243,\n",
      "               244,\n",
      "               245,\n",
      "               248,\n",
      "               251,\n",
      "               253,\n",
      "               255,\n",
      "               257,\n",
      "               261,\n",
      "               262,\n",
      "               268,\n",
      "               270,\n",
      "               271,\n",
      "               272,\n",
      "               273,\n",
      "               282,\n",
      "               287,\n",
      "               292,\n",
      "               293,\n",
      "               294,\n",
      "               295,\n",
      "               298,\n",
      "               304,\n",
      "               309,\n",
      "               313,\n",
      "               316,\n",
      "               324,\n",
      "               340,\n",
      "               345,\n",
      "               346,\n",
      "               347,\n",
      "               350,\n",
      "               353,\n",
      "               355,\n",
      "               362,\n",
      "               365,\n",
      "               370,\n",
      "               371,\n",
      "               379,\n",
      "               380,\n",
      "               382,\n",
      "               384,\n",
      "               388,\n",
      "               394,\n",
      "               402,\n",
      "               405,\n",
      "               406,\n",
      "               407,\n",
      "               410,\n",
      "               417,\n",
      "               418,\n",
      "               423,\n",
      "               425,\n",
      "               426,\n",
      "               428,\n",
      "               429,\n",
      "               430,\n",
      "               431,\n",
      "               433,\n",
      "               438,\n",
      "               441,\n",
      "               442,\n",
      "               446,\n",
      "               448,\n",
      "               450,\n",
      "               456,\n",
      "               458,\n",
      "               460,\n",
      "               461,\n",
      "               463,\n",
      "               468,\n",
      "               471,\n",
      "               472,\n",
      "               475,\n",
      "               478,\n",
      "               479,\n",
      "               480,\n",
      "               481,\n",
      "               483,\n",
      "               488,\n",
      "               500,\n",
      "               501,\n",
      "               503,\n",
      "               509,\n",
      "               519,\n",
      "               520,\n",
      "               521,\n",
      "               522,\n",
      "               523,\n",
      "               525,\n",
      "               527,\n",
      "               528,\n",
      "               531,\n",
      "               532,\n",
      "               542,\n",
      "               544,\n",
      "               547,\n",
      "               553,\n",
      "               554,\n",
      "               561,\n",
      "               562,\n",
      "               570,\n",
      "               572,\n",
      "               574,\n",
      "               575,\n",
      "               577,\n",
      "               580,\n",
      "               582,\n",
      "               584,\n",
      "               586,\n",
      "               587,\n",
      "               596,\n",
      "               598,\n",
      "               599,\n",
      "               607,\n",
      "               612,\n",
      "               613,\n",
      "               615,\n",
      "               620,\n",
      "               621,\n",
      "               622,\n",
      "               624,\n",
      "               627,\n",
      "               648,\n",
      "               654,\n",
      "               655,\n",
      "               660,\n",
      "               664,\n",
      "               665,\n",
      "               671]}\n"
     ]
    }
   ],
   "source": [
    "for item in cl.find({'movie_id': 4993}):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that there is an additional field called '\\_id' with ObjectId record. This is the unique key mongodb assigned to each document within collection.\n",
    "\n",
    "NoSQL 2: find product that both user 13 and 14 have bought before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_id': 2355}\n",
      "{'movie_id': 3114}\n"
     ]
    }
   ],
   "source": [
    "for item in cl.find({'user_list': {'$all':[13, 14]}}, {'user_list':0, '_id':0}):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NoSQL 3: find all movie_id that has been rated by equal or greater than 200 users, sort by movie_id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'movie_id': 1}\n",
      "{'movie_id': 47}\n",
      "{'movie_id': 50}\n",
      "{'movie_id': 110}\n",
      "{'movie_id': 150}\n",
      "{'movie_id': 260}\n",
      "{'movie_id': 296}\n",
      "{'movie_id': 318}\n",
      "{'movie_id': 356}\n",
      "{'movie_id': 364}\n",
      "{'movie_id': 457}\n",
      "{'movie_id': 480}\n",
      "{'movie_id': 527}\n",
      "{'movie_id': 588}\n",
      "{'movie_id': 589}\n",
      "{'movie_id': 590}\n",
      "{'movie_id': 593}\n",
      "{'movie_id': 608}\n",
      "{'movie_id': 780}\n",
      "{'movie_id': 858}\n",
      "{'movie_id': 1196}\n",
      "{'movie_id': 1198}\n",
      "{'movie_id': 1210}\n",
      "{'movie_id': 1270}\n",
      "{'movie_id': 2571}\n",
      "{'movie_id': 2858}\n",
      "{'movie_id': 2959}\n",
      "{'movie_id': 4993}\n"
     ]
    }
   ],
   "source": [
    "for item in cl.find({'user_list.199': {'$exists': 'true'}}, {'user_list':0, '_id':0}).sort('movie_id'):\n",
    "    pprint(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab3: Get Familiar with Apache Kafka & Spark Streaming\n",
    "\n",
    "This lab mainly helps you to know how to get Kafka and Spark Streaming work on cluster. You will first config Kafka and then run a simple Spark Streaming program to digest message in Kafka.\n",
    "\n",
    "** Run Kafka **\n",
    "\n",
    "[Apache Kafka](https://kafka.apache.org/quickstart) would manage our message queue for Spark Streaming. Since we install our cluster with Kafka initialization action, kafka has been installed in your cluster. ssh to recommend-m-0, All Kafka messages are organized into topics. If you wish to send a message you send it to a specific topic and if you wish to read a message you read it from a specific topic. Use the following command to create a topic named ratings, and keep in mind prefix each .sh command with directory '/usr/lib/kafka/bin/':\n",
    "~~~~\n",
    "kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic ratings\n",
    "~~~~\n",
    "'--create' means we want to create a topic, while '--topic' specifies topic name. Also, this command just talks to local zookeeper server for doing this creation function, and by default, the port for zookeeper server would always be 2181. Since our data is relatively small, you don't need to concern about '--replication-factor' and '--partitions',   just keep them as 1. You can view your current topics by\n",
    "~~~~\n",
    "kafka-topics.sh --list --zookeeper localhost:2181\n",
    "~~~~\n",
    "In Kafka, there are two threads --- producer and consumer. A consumer pulls messages off a Kafka topic while producers push messages into a Kafka topic. Also, since Kafka is a distributed system which runs on cluster, each node in the cluster is called a Kafka broker. By using --broker-list, you specify which machine to feed queue with message:\n",
    "~~~~\n",
    "kafka-console-producer.sh --broker-list recommend-w-0:9092 --topic ratings\n",
    "~~~~\n",
    "This command starts a console for feeding topic ratings and uses recommend-w-0 as broker to do so. open another terminal, type in the following consumer command, you will get message produced by recommend-w-0 from the queue one-by-one. --from-beginning ensures getting all the message in the history. Now you can type in any words to producer terminal, and magically, they will show up in your consumer terminal one-by-one.\n",
    "~~~~\n",
    "kafka-console-consumer.sh --bootstrap-server recommend-w-0:9092 --topic ratings --from-beginning\n",
    "~~~~\n",
    "Trust me, you don't want to type in ratings manually for debugging. With the help of shell operator '<', we could easily deal with it. Change into data/ directory, and type in the following command, you will see all rows in ratings_streaming_1.csv show in another terminal.\n",
    "~~~~\n",
    "kafka-console-producer.sh --broker-list recommend-w-0:9092 --topic ratings < streaming_1.csv\n",
    "~~~~\n",
    "\n",
    "** Command to run Kafka server locally **\n",
    "~~~~\n",
    "bin/zookeeper-server-start.sh config/zookeeper.properties\n",
    "bin/kafka-server-start.sh config/server.properties\n",
    "bin/kafka-topics.sh --zookeeper localhost:2181 --delete --topic ratings\n",
    "~~~~\n",
    "** Spark Streaming **\n",
    "\n",
    "For Spark Streaming, refer to [here](https://spark.apache.org/docs/latest/streaming-programming-guide.html). For connect Kafka and Spark Streaming, refer to [here](https://spark.apache.org/docs/latest/streaming-kafka-0-8-integration.html). There are two ways to connect them, we use direct approach (no receivers) to integrate Spark Streaming and Kafka. Not only because of all the advantages mentioned in the official website, but also because of a time exceed error for Spark Streaming to connect to zookeeper, since we initalize our cluster with 3 masters rather than preinstall zookeeper on cluster.\n",
    "\n",
    "Now let's take a look at streaming_test.py to understand how exactly should we build streaming pipeline. This code only reports new ratings about popular movies --- which already have more than 200 ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "from pyspark.sql import SparkSession, SQLContext\n",
    "import argparse\n",
    "\n",
    "#### util ##########################################################################################\n",
    "def getSparkSessionInstance(sparkConf):\n",
    "    if ('sparkSessionSingletonInstance' not in globals()):\n",
    "        globals()['sparkSessionSingletonInstance'] = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(\"data1030\") \\\n",
    "            .config(\"spark.mongodb.input.uri\", \"mongodb://jinyan:12345678@ds151973.mlab.com:51973/netflix\") \\\n",
    "            .config(\"spark.mongodb.output.uri\", \"mongodb://jinyan:12345678@ds151973.mlab.com:51973/netflix\") \\\n",
    "            .getOrCreate()\n",
    "\n",
    "    return globals()['sparkSessionSingletonInstance']\n",
    "\n",
    "#####################################################################################################\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description='test cloud setup')\n",
    "    parser.add_argument('-mongo', help='MongoDB database URI')\n",
    "    parser.add_argument('-b', help='broker list', default='recommend-w-0:9092')\n",
    "    return parser.parse_args()\n",
    "\n",
    "def mapper1(record):\n",
    "    key = record.movie_id\n",
    "    value = len(record.user_list)\n",
    "    return (key, value)\n",
    "\n",
    "def mapper2(record):\n",
    "    if (record[1] >= 200):\n",
    "        return [(record[0],)]\n",
    "    else:\n",
    "        return []\n",
    "\n",
    "def mapper3(record):\n",
    "    \"\"\"\n",
    "    :param record: \"user_id,movie_id,rating,timestamp\"\n",
    "    :return: (key, value)\n",
    "              key: user_id\n",
    "              value: movie_id\n",
    "    \"\"\"\n",
    "    mylist = record[1].split(\",\")\n",
    "    return (int(mylist[0]), int(mylist[1]))\n",
    "\n",
    "\n",
    "def process(rdd):\n",
    "    # this empty check is neccessary\n",
    "    if rdd.isEmpty():\n",
    "        return rdd\n",
    "\n",
    "    # get spark and spark streaming context, this is necessary for you to use SparkSQL\n",
    "    spark = getSparkSessionInstance(rdd.context.getConf())\n",
    "    ssc = SQLContext(spark.sparkContext)\n",
    "    \n",
    "    tmp_df = ssc.createDataFrame(rdd, ['user_id', 'movie_id'])\n",
    "    ssc.registerDataFrameAsTable(tmp_df, 'ratings')\n",
    "\n",
    "    return ssc.sql('''SELECT user_id, r.movie_id\n",
    "                   FROM ratings r, popular p\n",
    "                   WHERE r.movie_id == p.movie_id''').rdd\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    args = parse_args()\n",
    "    \n",
    "    # init spark session\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"data1030\") \\\n",
    "        .config(\"spark.mongodb.input.uri\", args.mongo) \\\n",
    "        .config(\"spark.mongodb.output.uri\", args.mongo) \\\n",
    "        .getOrCreate()\n",
    "    sc = spark.sparkContext\n",
    "    \n",
    "    # init spark streaming with batch interval of 10 seconds\n",
    "    stream = StreamingContext(sc, 10)\n",
    "    ssc = SQLContext(sc)\n",
    "    \n",
    "    # load data from MongoDB\n",
    "    watch_df = spark.read.format(\"com.mongodb.spark.sql.DefaultSource\")\\\n",
    "        .option(\"collection\", \"watch\")\\\n",
    "        .load()\n",
    "    watch_rdd = watch_df.rdd.map(mapper1).flatMap(mapper2)\n",
    "    watch_df_filtered = ssc.createDataFrame(watch_rdd, ['movie_id'])\n",
    "    ssc.registerDataFrameAsTable(watch_df_filtered, 'popular')\n",
    "\n",
    "    # streaming\n",
    "    kafka_stream = KafkaUtils.createDirectStream(stream, ['ratings'], {\"metadata.broker.list\": args.b})\n",
    "\n",
    "    tmp_rdd = kafka_stream.map(mapper3).transform(process)\n",
    "    tmp_rdd.pprint(10)\n",
    "\n",
    "    stream.start()\n",
    "    stream.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You maybe familiar with most code above, so let's focus on streaming part of it. As you may notice, we not only create SparkContext and SparkSQL context, but also have a streaming context. We use KafkaUtiles.createDirectStream() to connect to Kafka. The first parameter refers to streaming context, the second one is the topic, and the third one would be broker.list that this code may choose to listen. kafka_stream is DStream, which is a streaming version of normal RDD. Since some operation can't not be applied on it, we write most of our operation in transform method, within which we can apply all RDD operation. Keep in mind that you must print out something in order to make Spark Streaming run. At last, we use streaming.start() and awaitTermination() to make our program start to process streaming data. \n",
    "\n",
    "Now you can upload this code onto your cluster and use it with\n",
    "~~~~\n",
    "spark-submit --packages org.apache.spark:spark-streaming-kafka-0-8_2.11:2.2.0,org.mongodb.spark:mongo-spark-connector_2.11:2.2.0 streaming_test.py -mongo MongoDB_URI -b recommend-w-0:9092 > output\n",
    "~~~~\n",
    "Note: this is Java maven format --- groupId:artifactId:version. THe first package works for spark-kafka connector, while the second one works for spark-mongoDB connector. When we need to include multiple packages, using comma to sepearate them, without any breaks between them. If you use provided data streaming_1.csv, you will find output like this in your output file:\n",
    " <img src=\"document/streaming.jpeg\" width = \"400\" height = \"600\" alt=\"what\" align=center />\n",
    "It's beautiful, isn't it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Part 1: Batch Data Pipeline by Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assginment, you will achieve a movie recommenation system based on Amazon.com recommendation system. Below is the workflows that we will implement. Figure 2 shows the algorithm for streaming, while figure 3 shows the algorithm for batch. The following instruction will help you understand each steps of these two workflow, and how you should implement them in MapReduce. We will use the number beside each rectangle to denote steps. Feel free to use code we provided before, they are really useful.\n",
    "\n",
    " <img src=\"document/workflow.jpeg\" width = \"1000\" height = \"800\" alt=\"what\" align=center />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from implementing figure 3! \n",
    "\n",
    "** before 100 **\n",
    "\n",
    "You need to init Spark context and SparkSQL context with mongodb connecting information, refer to [MongoDB-Spark Connector](https://docs.mongodb.com/spark-connector/master/python-api/).\n",
    "\n",
    "** 100 **\n",
    "\n",
    "In our case, purchase history refers to user ratings for each movie. \n",
    "\n",
    "1. Load data. You should first use [HDFS shell](https://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#put) to put data/ratings.csv and data/movies.csv into Hadoop Distributed File System at cluster. We suggest putting them in to directory 'hdfs:///data/'. Note that 'hdfs://' denotes HDFS, while /data/ is the directory in it. Use spark [read function](https://docs.databricks.com/spark/latest/data-sources/read-csv.html) to load data into your Spark code.\n",
    "2. Create movie collection in MongoDB. In this step, you need to use previous data to create table that stores average rating and movie title for each movie. We recommand you to use SparkSQL for this step --- first register two temporary tables based on previous data, use standard SQL to generate movie tables, and then write data into MongoDB with [write function](https://docs.mongodb.com/spark-connector/master/python/write-to-mongodb/).\n",
    "\n",
    "** 102 **\n",
    "Our algorithm doesn't need this step.\n",
    "\n",
    "** 104 **\n",
    "\n",
    "You can easily create a rdd that maps purchased item to users. The format of this rdd should be:\n",
    "\n",
    "~~~~\n",
    "(key, value)\n",
    " key: movie_id\n",
    " value: list of (user_id, rating) tuples\n",
    "~~~~\n",
    "** 106 **\n",
    "\n",
    "We set global value POPULAR_THRESHOLD (200) to indicate threshold for popular movie, that is, if a movie have been rated by equal or greater than POPULAR_THRESHOLD users, then it can be viewed as popular movie. You should fill popular_item list with popular movie id. \n",
    "\n",
    "** 108 **\n",
    "\n",
    "This one is a little bit tricky. We can use two map functions and two reduce functions. Mapper1 simply generates key value pair where user_id is the key and all the other information as value. Reduce it by key, we now get list for all movies that a user has rated. Mapper2 generates tuple of each popular movie - other movie pair, with  number of rating users for each movie as its value. Reduce it by key, now each item in value list corresponds to one common user who rates both movies in key tuple.\n",
    "~~~~\n",
    "mapper1(record):\n",
    "    input: (key, value)\n",
    "            key: movie_id\n",
    "            value: list of (user_id, rating) tuples\n",
    "    output: list of (key, value) pair\n",
    "            key: user_id\n",
    "            value: [(movie_id, rating, number of rater)]\n",
    "mapper2(record):\n",
    "    input: (key, value)\n",
    "            key: user_id\n",
    "            value: list of (movie_id, rating, number of rater)\n",
    "    output: list of (key, value) tuple\n",
    "            key: (movie_id1, movie_id2)\n",
    "            value: ([(num_rater1, num_rater2)])\n",
    "~~~~\n",
    "** 110 & 114 **\n",
    "\n",
    "We will flip 112 and 114 steps. Our algorithm uses Jaccard similarity to measure similarity between movies and only return movie pairs that share **more than** RATING_NUM (3) common users. \n",
    "~~~~\n",
    "mapper3(record):\n",
    "    input: (key, value)\n",
    "            key: (movie_id1, movie_id2)\n",
    "            value: list of (num_rater1, num_rater2) pair, where length of this list equal to number of common users\n",
    "    output: [(key, value)] or []\n",
    "            key: movie_id\n",
    "            value: [(movie_id2, jaccard)]\n",
    "            if they doesn't satisfy RATING_NUM constrain, return []\n",
    "~~~~\n",
    "** 112 & 116 **\n",
    "\n",
    "Now we need to sort other item lists for each popular item and truncate this list to n (3), which is a global variable already defined at the top of our code. \n",
    "~~~~\n",
    "mapper4(record):\n",
    "    input: (key, value)\n",
    "            key: movie_id1\n",
    "            value: [(movie_id2, jaccard), ...]\n",
    "    output:(movie_id1, (movie_id, jaccard), (movie_id, jaccard), ...)\n",
    "            sorted tuples with n other items\n",
    "~~~~\n",
    "\n",
    "** after 116 **\n",
    "\n",
    "Until now, you have achieved all the steps in batch workflow. we now need to upload this similar table into MongoDB. Use the same write function in step 100, and name your collection similar. Make sure each document in your collection has the following schema:\n",
    "~~~~\n",
    "{'_id': ObjectId, 'popular_id': int, 'item_1': {'_1': int, '_2': float}, ..., 'item_n':{...}}\n",
    "~~~~\n",
    "Now you can check your result with the following NoSQL. The result should be the same with problem 3 at NoSQL lab 3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'popular_id': 1}\n",
      "{'popular_id': 47}\n",
      "{'popular_id': 50}\n",
      "{'popular_id': 110}\n",
      "{'popular_id': 150}\n",
      "{'popular_id': 260}\n",
      "{'popular_id': 296}\n",
      "{'popular_id': 318}\n",
      "{'popular_id': 356}\n",
      "{'popular_id': 364}\n",
      "{'popular_id': 457}\n",
      "{'popular_id': 480}\n",
      "{'popular_id': 527}\n",
      "{'popular_id': 588}\n",
      "{'popular_id': 589}\n",
      "{'popular_id': 590}\n",
      "{'popular_id': 593}\n",
      "{'popular_id': 608}\n",
      "{'popular_id': 780}\n",
      "{'popular_id': 858}\n",
      "{'popular_id': 1196}\n",
      "{'popular_id': 1198}\n",
      "{'popular_id': 1210}\n",
      "{'popular_id': 1270}\n",
      "{'popular_id': 2571}\n",
      "{'popular_id': 2858}\n",
      "{'popular_id': 2959}\n",
      "{'popular_id': 4993}\n"
     ]
    }
   ],
   "source": [
    "cl = db.similar\n",
    "for item in cl.find({}, {'popular_id': 1, '_id': 0}).sort('popular_id'):\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tips on Debug **\n",
    "1. set POPULAR_THRESHOLD big\n",
    "2. set n big\n",
    "3. when submit your task, use '... > output' to separate system INFO and program output\n",
    "4. use rdd.take(number) to show # of items in rdd\n",
    "5. Feel free to optimize any code part, and have a writeup about your optimization in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Real-time Data Pipeline by Spark Streaming\n",
    "\n",
    "The structure of Spark Streaming has been introduced in lab3. In this part, we will implement figure 2 with them. Feel free to copy and paste lab3 code into your streaming program. \n",
    "\n",
    "** pre 80 **\n",
    "\n",
    "1. Establish connection with Spark, Spark Streaming with interval to be 10 seconds, SparkSQL. Initialize pymongo client again, and notice that we declare variable client at the top of code because we may use it in process function.\n",
    "2. We need to use similar and movies collections generated in batch pipeline. So, use spark.read function to load these two collections from MongoDB, and register them as tables with the same name in sparkSQL.\n",
    "3. Initialize recommend collection in MongoDB with the following schema:\n",
    "~~~~\n",
    "['user_id': int, 'movie_list': array of integer, 'recommend_list': array of integer]\n",
    "~~~~\n",
    "and also register a table in sparkSQL with the same name and schema. user_id should be in range [0, 600], will movie_list and recommend_list should be empty.\n",
    "\n",
    "*** All the following process would only be done to users corresponding to the new comming rating data***\n",
    "\n",
    "** 80 **\n",
    "\n",
    "In our recommendation system, we simply regard all items that one user has rated as items that known to be of interest to user. \n",
    "1. update recommend collection using pymongo. Insert new rated movie id into movie_list of corresponding user. You need to check for duplication, i.e., movie_list shouldn't have duplicate id.\n",
    "2. retrive user_id and movie_list from recommend collection. Register it as a table in SparkSQL with same name.\n",
    "\n",
    "** 82 **\n",
    "\n",
    "Retrive similar movies for all movies known of interest for each user.\n",
    "1. Create a table named input_rating with schema ['user_id': int, 'movie_id': int], where each row corresponding to a user and one movie of his interest. \n",
    "2. Join input_rating table with similar table to find out all similar items. The output should be:\n",
    "\n",
    "~~~~\n",
    "(user_id, list of similar movie_id)\n",
    "~~~~\n",
    "\n",
    "** 84 ** Our algorithm doesn't need this step\n",
    "\n",
    "** 86 **\n",
    "\n",
    "Use MapReduce function to merge item list. Note that you should deduplicate this list. The output of this step should be rdd with \n",
    "\n",
    "~~~~\n",
    "(user_id, movie_id).\n",
    "~~~~\n",
    "\n",
    "** 88 **\n",
    "\n",
    "We will sort similar items by their average rating score according to table movies. In order to simplify further operation, you need to do some additional work here:\n",
    "1. register a table named user_new with previous rdd in SparkSQL\n",
    "2. join user_new and movies to get dataframe of (user_id, movie_id, rating_avg, title)\n",
    "3. use MapRecue function to descending sort similar movies for each user. The output should be rdd with format:\n",
    "\n",
    "~~~~\n",
    "(user_id, sorted list of (movie_id, avg_rating, movie_title))\n",
    "~~~~\n",
    "\n",
    "** 90 **\n",
    "\n",
    "Filter and truncate recommend list. You should filter out movies that already been rated by user, i.e., movies in movie_list. Truncate recommend list to RECOMMEND_LENGTH. The output should be:\n",
    "\n",
    "~~~~\n",
    "(user_id, filtered and truncated list of (movie_id, movie_title))\n",
    "~~~~\n",
    "\n",
    "** 92 **\n",
    "\n",
    "Simply set recommend_list in recommend collection to your current recommendation list generated in step 90, with (movie_id, movie_title) for each movie.\n",
    "\n",
    "** 94 **\n",
    "\n",
    "You need to return the final rdd in process function back to main function, and use stream.pprint() function to print them out. Number 10 would be sufficient for pprint.\n",
    "\n",
    "** test your code **\n",
    "\n",
    "We provided two incoming rating file, named streaming_1.csv and streaming_2.csv. You should test your streaming code by feeding those two files to Kafka-producer. After you feed in streaming_1.csv, your recommend collection should look like this:\n",
    " <img src=\"document/streaming_1.jpeg\" width = \"800\" height = \"700\" alt=\"what\" align=center />\n",
    "and your recommendation system should print out following recommendation information:\n",
    "\n",
    "~~~~\n",
    "(6, [(595, 'Beauty and the Beast (1991)'), (592, 'Batman (1989)'), (597, 'Pretty Woman (1990)')])\n",
    "(13, [(7153, 'Lord of the Rings: The Return of the King, The (2003)'), (5952, 'Lord of the Rings: The Two Towers, The (2002)'), (6539, 'Pirates of the Caribbean: The Curse of the Black Pearl (2003)')])\n",
    "~~~~\n",
    " \n",
    "After you feed in streaming_2.csv, recommend collection should look like:\n",
    " <img src=\"document/streaming_2.jpeg\" width = \"800\" height = \"700\" alt=\"what\" align=center />\n",
    "with following output:\n",
    "\n",
    "~~~~\n",
    "(6, [(595, 'Beauty and the Beast (1991)'), (480, 'Jurassic Park (1993)'), (380, 'True Lies (1994)')])\n",
    "(14, [(1196, 'Star Wars: Episode V - The Empire Strikes Back (1980)'), (1198, 'Raiders of the Lost Ark (Indiana Jones and the Raiders of the Lost Ark) (1981)'), (1210, 'Star Wars: Episode VI - Return of the Jedi (1983)')])\n",
    "~~~~"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tips for Debugging **\n",
    "1. When using DStream.transform(func), you need to return rdd in func otherwise all the succeed rdd would be empty!\n",
    "2. Sometimes program will print duplicate rows, that would be fine if your code is generic.\n",
    "3. Removeing temporary table is a good habit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
